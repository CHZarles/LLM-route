CH2:

attention 感性介绍, 这个文章介绍了单个 Attention/self.Attention 的机制，理解了这个再去看多头attention会比较好
https://www.zywvvd.com/notes/study/deep-learning/transformer/transformer-intr/transformer-intr-1/

这个讲了Transformer里面的 self-attention
https://www.zywvvd.com/notes/study/deep-learning/transformer/transformer-intr/transformer-intr-2/

补充embedding encode层：
https://www.zywvvd.com/notes/study/deep-learning/transformer/transformer-intr/transformer-intr-3/

pytorch 里面的varibable是什么：
https://cloud.baidu.com/article/2488084

向量嵌入：
https://www.elastic.co/cn/what-is/vector-embedding

向量序列：

词元：

block1:

反向传播：

梯度消失

梯度爆炸

残差连接

PE 矩阵

收集的资料：
【强化学习的数学原理】课程：从零开始到透彻理解（完结）[https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0&vd_source=27d3b33a76014ebb5a906ad40fa382de]
